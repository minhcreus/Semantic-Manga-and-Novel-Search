import streamlit as st
import pandas as pd
import numpy as np
import faiss
import re
import nltk
from sentence_transformers import SentenceTransformer
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TreebankWordTokenizer

# --- Setup and Configuration ---

# NLTK setup (run once and cache)
@st.cache_resource
def setup_nltk():
    try:
        nltk.data.find("tokenizers/punkt")
    except LookupError:
        nltk.download("punkt", quiet=True)
    try:
        nltk.data.find("corpora/wordnet")
    except LookupError:
        nltk.download("wordnet", quiet=True)

setup_nltk()

# Load model (run once and cache)
@st.cache_resource
def load_model():
    return SentenceTransformer("all-MiniLM-L6-v2")

model = load_model()

# --- Data Loading ---
# Load the pre-processed data and embeddings.
# Using st.cache_data to prevent reloading on every interaction.
@st.cache_data
def load_data():
    """Loads the cleaned data and embeddings from disk."""
    try:
        # Load the CSV generated by the create_embeddings.py script
        df = pd.read_csv("cleaned_manga_data.csv")
        # Load the corresponding embeddings
        embeddings = np.load("manga_novel_embeddings.npy")
        return df, embeddings
    except FileNotFoundError:
        st.error("Error: `cleaned_manga_data.csv` or `manga_novel_embeddings.npy` not found.")
        st.error("Please run the `create_embeddings.py` script first to generate these files.")
        return None, None

df, embeddings = load_data()

# Stop the app if data loading failed
if df is None:
    st.stop()

# For display purposes, capitalize column names (they are lowercase in the cleaned file)
df.columns = [col.strip().capitalize() for col in df.columns]

# --- Pre-computation for Filtering ---
# Prepare genre list for the multiselect widget
has_genre = "Genre" in df.columns
unique_genres = sorted(set(g.strip() for g_list in df['Genre'].dropna() for g in str(g_list).split(','))) if has_genre else []


# --- Text Processing and Search Functions ---

lemmatizer = WordNetLemmatizer()
tokenizer = TreebankWordTokenizer()

def normalize_query(query):
    """Lemmatizes and lowercases the user query."""
    tokens = tokenizer.tokenize(query.lower())
    lemmatized = [lemmatizer.lemmatize(t) for t in tokens if re.match(r'\w+', t)]
    return ' '.join(lemmatized)

def highlight(text, query):
    """Highlights the query terms in the result text."""
    if not isinstance(text, str):
        return "" # Return empty string if text is not a string (e.g., NaN)
    query_words = map(re.escape, query.split())
    pattern = re.compile(r'(' + '|'.join(query_words) + r')', re.IGNORECASE)
    return pattern.sub(r'**\1**', text)

def semantic_search(query, df, embeddings, model, selected_genres=None, top_k=5):
    """
    Performs semantic search using FAISS for efficiency.
    Filters by genre before searching.
    """
    # 1. Filter data based on selected genres
    if has_genre and selected_genres and 'All' not in selected_genres:
        # Create a boolean mask for genres using original DataFrame indices
        genre_mask = df['Genre'].str.contains('|'.join(map(re.escape, selected_genres)), na=False)
        sub_df = df[genre_mask]
        
        if sub_df.empty:
            return pd.DataFrame() 
        
        # Select embeddings corresponding to the filtered dataframe
        sub_embeddings = embeddings[sub_df.index]
        sub_df = sub_df.reset_index(drop=True) # Reset index for mapping FAISS results
    else:
        sub_df = df
        sub_embeddings = embeddings

    if sub_df.empty or len(sub_embeddings) == 0:
        return pd.DataFrame()

    # 2. Build a FAISS index for the relevant embeddings
    # Our embeddings are pre-normalized, so we use IndexFlatIP for cosine similarity
    d = sub_embeddings.shape[1]
    index = faiss.IndexFlatIP(d)
    index.add(sub_embeddings.astype('float32'))

    # 3. Encode the query and search the index
    normalized_query = normalize_query(query)
    query_embedding = model.encode([normalized_query], normalize_embeddings=True).astype('float32')
    
    distances, indices = index.search(query_embedding, top_k)

    # 4. Format and return results
    results_df = sub_df.iloc[indices[0]].copy()
    results_df['Score'] = distances[0]
    
    return results_df

# --- Streamlit UI ---

st.set_page_config(layout="wide")
st.title("ðŸ“š Novel & Manga Semantic Search Engine")

query = st.text_input("Enter a query (e.g., apocalypse, reincarnation, cultivation):", placeholder="Search for a story about a hero returning to the past...")

col1, col2 = st.columns([3, 1])
with col1:
    if unique_genres:
        selected_genres = st.multiselect("Filter by Genre:", options=['All'] + unique_genres, default=['All'])
    else:
        selected_genres = [] # No genre filter if column doesn't exist
with col2:
    top_k = st.slider("Number of results:", min_value=1, max_value=20, value=5)

if query:
    results = semantic_search(query, df, embeddings, model, selected_genres=selected_genres, top_k=top_k)

    if results.empty:
        st.warning("No results found for your query or genre selection.")
    else:
        st.write(f"Showing top {len(results)} results:")
        for i, row in results.iterrows():
            st.markdown("---")
            st.markdown(f"### {row.get('Title', 'No Title')}")
            
            meta_col1, meta_col2 = st.columns(2)
            with meta_col1:
                st.markdown(f"**Similarity Score:** `{row['Score']:.4f}`")
                if pd.notna(row.get("Link")):
                    st.markdown(f"**Source:** [Read Here]({row['Link']})")
            with meta_col2:
                 if pd.notna(row.get("Genre")):
                    st.markdown(f"**Genres:** `{row['Genre']}`")

            summary_text = row.get("Summary", "No summary available.")
            st.markdown(highlight(summary_text, query), unsafe_allow_html=True)
else:
    st.info("Enter a query above to search for novels and manga.")
